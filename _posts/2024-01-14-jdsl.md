---
layout: post
title: "How I Accidentally Created the “JDSL” of Data Pipelines - And It's Awesome"
categories: [JSON, Data Pipeline]
permalink: /posts/how-i-accidentally-created-the-jdsl-of-data-pipelines
---

I ended up creating a data pipeline tool where everything is programmed in JSON, becoming the “[JDSL](https://www.youtube.com/watch?v=QwUPs5N9I6I)” of data processing. And I promise, it's actually quite effective!

## But Why Did I Create This?

It all started at a solar company where I was the ML Engineer, responsible for creating ML models that operated on streaming data.

However, we hadn't implemented any robust tools for creating data pipelines, and I was the only one tasked with addressing this issue. This might have been the point where I should have opted for a well-known data processing tool like Spark or Flink, but then came the challenge of training-serving skew in ML.

This skew occurs when an ML model, trained on one machine, runs on another, potentially using a different programming language. This means data transformations need to be replicated. For example, a training job might process data in SQL and Python, while production might use Scala and Python. Replicating complex mathematical functions can quickly lead to slightly different results, rendering the ML model ineffective and essentially useless.

Therefore, I needed a way to ensure the transformations were aligned on every machine, regardless of the programming language or version they were running on.

## The Crazy Idea

This is where I had my crazy idea. As each line of data transformation can be represented with very little information. 
For instance, if we want to multiply the “amount” column by 10, we can represent a “multiply column with constant value” transformation and provide the column name (“amount”) and value (10).
The same concept applies to adding two columns - we just need to know which two columns to use.
Or, for something more advanced, like generating an embedding, we need to know which embedding model to use, and for which column.

In this way, each transformation would be technically agnostic, and therefore shareable across machines.

With this in mind, I needed a way to describe these transformations. This led to the use of JSON. Meaning, we actually program ETL pipelines by defining JSON! And suddenly, we ended up as the [JDSL](https://www.youtube.com/watch?v=QwUPs5N9I6I) of data pipelines.

```
{
    "name": "is_female",
    "transformation": {
        "name": "equals", 
        "key": "sex", 
        "value": {"name": "string", "value": "female"}
    },
    "depending_on": [
        {"name": "sex", ...}
    ],
    ...
}
```

## JSON Was a Bit Too Crazy

However, programming directly in JSON is not practical. It would lead to many frustrating formatting errors. There was no linting to check that the file was semantically correct, and no code completion to indicate which parameters should be set.

Therefore, I also set up a Python API that provides type-safe transformations, code completion, and then compiles everything to the JSON file.

```python
from aligned import Int32, Float, String, feature_view

@feature_view(...)
class TitanicPassenger:    
    passenger_id = Int32()
        
    sex = String().accepted_values(["male", "female"])
    is_male, is_female = sex.one_hot_encode(['male', 'female'])
    
    age = Float().description("Contains some 1.2 values, so needs to be a float")
    
    sibsp = Int32().description("Number of siblings on the Titanic")
    has_siblings = sibsp > 0
```

Notice that we are not referencing columns using strings, but rather using the fields themselves! This leads to both code completion and type-safety, as linters can catch errors.

## It Was Way Better Than What I Thought

Even though the data in the JSON file is useless by itself, there's a lot of valuable information in there. Applying this information correctly can lead to great results. We can gain the following functionality for free:

- Data Validation (both data types and semantic checks)
- Data Lineage Graphs
![View Data Lineage](/assets/videos/aligned-overall-data-lineage.mp4)
- Data Catalogs
![View Data Catalog](/assets/videos/aligned-data-catalog.mp4)
- Data Freshness Checks
- Continuing Pipelines from Cached States
- Incremental Data Materialization
- Warning about Data Migration Conflicts
- Debug spesific feature transformations
![View Data Catalog](/assets/videos/aligned-test-transformations.mp4)

And that is only the data engineering features, and not taking into account the MLOps features. 

But there's so much more! Having a file that contains all these transformations, input features, etc., in a technology-agnostic way has led to an incredibly flexible system. It has enabled me to move much faster than I previously thought possible.

Just look at this data catalog I managed to set up in what felt like just one week!

So why not give [aligned](https://github.com/MatsMoll/aligned) a try and see what you think?
